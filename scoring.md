Overview:
In our evaluation script score.py, we pass our set of chatbot answers generated by our model along with their corresponding golden standard answers. Then, we receive a score from (1-5), where 1 is the lowest and 5 is the highest possible score, for each chatbot answer, which reflects how semantically similar it is to its golden answer. Once every chatbot answer is scored, we receive a final score between 0 and 1, where trials that receive a score closer to 0 consist lower scores, such as 1 and 2, more often, and those with scores closer to 1 consist higher scores, such as 4 and 5 more often. Please refer to our report for a deeper dive into how the evaluation metric works.


How to Run:
To run score.py, use the following command line scheme:

python score.py <name of csv file with column names 'chatbot_answer' and 'golden_standard_answer'>

The final outputs will have the following structure:

EVALUATION
CHATBOT ANSWER <chatbot answer from csv file>    SCORE: <score 1-5> / 5
.....
TOTAL SCORE (0-1): <total score>

See the example below for what an expected csv file will look like and its output.


Example:

example.csv

chatbot_answer, golden_standard_answer
Michael Kearns, Michael Kearns
I do not know the answer to that, I don't know
Michael Kearns, Mark Yatskar

command (example.csv is in the same folder as score.py): python score.py example.csv

output:

EVALUATION
CHATBOT ANSWER Michael Kearns    SCORE: 5 / 5
CHATBOT ANSWER I do not know the answer to that    SCORE: 5 / 5
CHATBOT ANSWER Michael Kearns    SCORE: 2 / 5
TOTAL SCORE (0-1): .802


Sources:
Liu, Y., Iter, D., Yichong, X., Shuohang, W., Ruochen, X., Cheunguang, Z., G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment, https://doi.org/10.48550/arXiv.2303.16634

Kocmi, T., Federmann, C., Large Language Models Are State-of-the-Art Evaluators of Translation Quality, https://doi.org/10.48550/arXiv.2302.14520
