{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGpNmbBRPRGX"
      },
      "outputs": [],
      "source": [
        "#!pip install openai datasets\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from time import sleep\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = ''"
      ],
      "metadata": {
        "id": "KA8JA8CNP8Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "-bkWQpIzjj5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},"
      ],
      "metadata": {
        "id": "bzha3IEXaSzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cache = {}\n",
        "def run_gpt3(prompt, return_first_line = True, instruction_tuned = False):\n",
        "    # Return the response from the cache if we have already run this\n",
        "    cache_key = (prompt, return_first_line, instruction_tuned)\n",
        "    if cache_key in cache:\n",
        "        return cache[cache_key]\n",
        "    client = OpenAI(\n",
        "      api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        "    )\n",
        "\n",
        "\n",
        "    # Select the model\n",
        "    if instruction_tuned:\n",
        "        model = \"gpt-3.5-turbo-0613\"\n",
        "    else:\n",
        "        model = \"gpt-3.5-turbo-0613\"\n",
        "\n",
        "    # Send the prompt to GPT-3\n",
        "    for i in range(0,60,6):\n",
        "        try:\n",
        "            #response = client.chat.completions.create(\n",
        "            # response = client.completions.create(\n",
        "            #     model=model,\n",
        "            #     # temperature=0,\n",
        "            #     # max_tokens=100,\n",
        "            #     # prompt=prompt,\n",
        "            #     # messages=[\n",
        "            #     #     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            #     #     {\"role\": \"system\", \"content\": prompt}],\n",
        "            #     # top_p=1,\n",
        "            #     # frequency_penalty=0.0,\n",
        "            #     # presence_penalty=0.0,\n",
        "            # )\n",
        "            response = client.chat.completions.create(\n",
        "                messages=[\n",
        "                        {\n",
        "                          \"role\": \"user\",\n",
        "                          \"content\": \"Say this is a test\",\n",
        "                        }\n",
        "                        ],\n",
        "             model='gpt-3.5-turbo'\n",
        "            )\n",
        "            response = dict(response)['choices'][0]\n",
        "            response = dict(response)['text'].strip()\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            sleep(i)\n",
        "\n",
        "    # Parse the response\n",
        "    if return_first_line:\n",
        "        final_response = response.split('\\n')[0]\n",
        "    else:\n",
        "        final_response = response\n",
        "\n",
        "    # Cache and return the response\n",
        "    cache[cache_key] = final_response\n",
        "    return final_response"
      ],
      "metadata": {
        "id": "-GklI8uTQIQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from time import sleep\n",
        "# import openai\n",
        "\n",
        "# # Assuming OPENAI_API_KEY is defined elsewhere in your code\n",
        "\n",
        "# cache = {}\n",
        "\n",
        "# def run_gpt3(prompt, return_first_line=True, instruction_tuned=False):\n",
        "#     # Return the response from the cache if we have already run this\n",
        "#     cache_key = (prompt, return_first_line, instruction_tuned)\n",
        "#     if cache_key in cache:\n",
        "#         return cache[cache_key]\n",
        "\n",
        "#     # Select the model based on instruction_tuned parameter\n",
        "#     model = \"gpt-3.5-turbo-1106\" if not instruction_tuned else \"gpt-3.5-turbo-16k\"\n",
        "\n",
        "#     # Send the prompt to GPT-3 using the chat completions endpoint\n",
        "#     for i in range(0, 60, 6):\n",
        "#         try:\n",
        "#             response = client.chat.completions.create(\n",
        "#                 model=model,\n",
        "#                 messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#                           {\"role\": \"user\", \"content\": prompt}],\n",
        "#                 temperature=0,\n",
        "#                 max_tokens=100,\n",
        "#                 top_p=1,\n",
        "#                 frequency_penalty=0.0,\n",
        "#                 presence_penalty=0.0,\n",
        "#             )\n",
        "\n",
        "#             response = response['choices'][0]['message']['content'].strip()\n",
        "#             break\n",
        "#         except Exception as e:\n",
        "#             print(e)\n",
        "#             sleep(i)\n",
        "\n",
        "#     # Parse the response\n",
        "#     if return_first_line:\n",
        "#         final_response = response.split('\\n')[0]\n",
        "#     else:\n",
        "#         final_response = response\n",
        "\n",
        "#     # Cache and return the response\n",
        "#     cache[cache_key] = final_response\n",
        "#     return final_response"
      ],
      "metadata": {
        "id": "OY-NWsMTW6aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot_sentence = 'I think Professor Ming would be good'\n",
        "golden_answer = 'Professor Ming would be a good option'"
      ],
      "metadata": {
        "id": "jy2WQOquQ5Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt that contains the definition of the evaluation task and the desired evaluation criteria\n",
        "#includes COT\n",
        "TASK_PROMPT = \"You will be given an answer generated by a chatbot and a golden standard answer. \\\n",
        "Your task is to rate the chatbot generated answer based on how similar it is to the \\\n",
        "golden standard answer and how coherent the chatbot answer is. \\\n",
        "Please make sure to read and understand these instructions carefully below:\\n \\\n",
        "\\n\\\n",
        "Evaluation Criteria :\\n \\\n",
        "Similarity (0-5) :\\n  \\\n",
        "Coherence (0-5) :\\n \\\n",
        "\\n \\\n",
        "Evaluation Steps :\\n \\\n",
        "1. Read the golden standard answer carefully and make sure to identify it's main points.\\n \\\n",
        "2. Read the chatbot answer and compare it to the golden standard answer. \\\n",
        "Check that both answers have similar context, meaning that they suggest the same thing,  \\\n",
        "and that the chatbot answer is presented in a clear and logical way.\\n \\\n",
        "3. Assign a score for similarity on a scale of 0 to 5, where 0 is the lowest and 5 is the \\\n",
        "highest on the based on the Evaluation Criteria.\\n \\\n",
        "4. Assign a score for coherence on a scale of 0 to 5, where 0 is the lowest and 5 is the \\\n",
        "highest on the based on the Evaluation Criteria.\\n \\\n",
        "5. Take the sum of the two scores so that way the score is between 0 and 10. \\\n",
        "6. Output the score.\\n \\\n",
        "\\n \\\n",
        "The chatbot answer is: {input_1}\\n \\\n",
        "The golden standard answer is: {input_2}\\n \\\n",
        "The score is : \""
      ],
      "metadata": {
        "id": "cRUJ7H5ORG8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_complete = TASK_PROMPT.replace(\"{input_1}\", chatbot_sentence).replace(\"{input_2}\", golden_answer)\n",
        "print(prompt_complete)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c41TGav351A",
        "outputId": "d1eae354-76ed-42a0-a36f-ac1f5e032b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You will be given an answer generated by a chatbot and a golden standard answer. Your task is to rate the chatbot generated answer based on how similar it is to the golden standard answer and how coherent the chatbot answer is. Please make sure to read and understand these instructions carefully below:\n",
            " \n",
            "Evaluation Criteria :\n",
            " Similarity (0-5) :\n",
            "  Coherence (0-5) :\n",
            " \n",
            " Evaluation Steps :\n",
            " 1. Read the golden standard answer carefully and make sure to identify it's main points.\n",
            " 2. Read the chatbot answer and compare it to the golden standard answer. Check that both answers have similar context, meaning that they suggest the same thing,  and that the chatbot answer is presented in a clear and logical way.\n",
            " 3. Assign a score for similarity on a scale of 0 to 5, where 0 is the lowest and 5 is the highest on the based on the Evaluation Criteria.\n",
            " 4. Assign a score for coherence on a scale of 0 to 5, where 0 is the lowest and 5 is the highest on the based on the Evaluation Criteria.\n",
            " 5. Take the sum of the two scores so that way the score is between 0 and 10. 6. Output the score.\n",
            " \n",
            " The chatbot answer is: I think Professor Ming would be good\n",
            " The golden standard answer is: Professor Ming would be a good option\n",
            " The score is : \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot_sentence_1 = 'Professor Ming is the best professor'\n",
        "golden_answer_1 = 'The best professor is Professor Ming'"
      ],
      "metadata": {
        "id": "76QNdamS6kQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot_sentence_2 = 'A good option is Professor S'\n",
        "golden_answer_2 = 'Professor Ming would be a good option'"
      ],
      "metadata": {
        "id": "6IZ4o-xEHuyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TASK_PROMPT_1 = \"Your task to return a score that ranges from 1 to 5 (where 1 is the lowest and 5 is the highest score)\\\n",
        "based on how close the chatbot \\\n",
        "and golden standard answer are in meaning and if they give the same answer. \\\n",
        "The chatbot answer is {input_1} and the golden standard answer is {input_2}. The returned score is: \""
      ],
      "metadata": {
        "id": "ZBInmQJDF0Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TASK_PROMPT_1 = \"\"\"\n",
        "Given a chatbot answer and golden standard answer, \\\n",
        "your task is to determine how close they are in meaning. \\\n",
        "Please make sure to read and understand all of the instructions carefully below:\\\n",
        "\n",
        "Evaluation Criteria (Read all of the options first and then assign the score that best describes \\\n",
        "the relationship between the chatbot and golden standard answer):\\\n",
        "\n",
        "Similarity (1-5):\\\n",
        "\n",
        "- Return a 1 if the chatbot answer doesn't return anything valid.\\\n",
        "- Return a 2 if the chatbot answer gives an answer but it's not necessarily valid or close to the golden answer.\\\n",
        "- Return a 3 if the chatbot answer gives a suggestion that is far from the golden answer.\\\n",
        "- Return a 4 if the chatbot answer gives a suggestion that is pretty similar to the golden standard answer but not in the expected order.\\\n",
        "- Return a 5 if the chatbot answer means the same thing as the golden standard answer, regardless of word order.\\\n",
        "\n",
        "Evaluation Steps:\\\n",
        "\n",
        "1. Read the golden standard answer and the chatbot answer carefully.\\\n",
        "2. Compare the chatbot answer to the golden standard answer. Check if both answers convey the same meaning.\\\n",
        "3. Assign a score for similarity on a scale of 1 to 5, where 1 is the lowest score and 5 is the highest possible score based on the Evaluation Criteria.\\\n",
        "4. Output the score.\\\n",
        "\n",
        "The chatbot answer is: {input_1}\\\n",
        "The golden standard answer is: {input_2}\\\n",
        "The score that best describes the similarity from 1 to 5 is:\n",
        "\n",
        "Given a chatbot answer and golden standard answer, \\\n",
        "your task is to determine how close they are in meaning. \\\n",
        "Please make sure to read and understand all of the instructions carefully below:\\\n",
        "\n",
        "Evaluation Criteria (Read all of the options first and then assign the score that best describes \\\n",
        "the relationship between the chatbot and golden standard answer):\\\n",
        "\n",
        "Similarity (1-5):\\\n",
        "\n",
        "- Return a 1 if the chatbot answer doesn't return anything valid.\\\n",
        "- Return a 2 if the chatbot answer gives an answer but it's not necessarily valid or close to the golden answer.\\\n",
        "- Return a 3 if the chatbot answer gives a suggestion that is far from the golden answer.\\\n",
        "- Return a 4 if the chatbot answer gives a suggestion that is pretty similar to the golden standard answer but not in the expected order.\\\n",
        "- Return a 5 if the chatbot answer and the golden standard answer convey the same meaning, even if the words or word order are different.\\\n",
        "\n",
        "Evaluation Steps:\\\n",
        "\n",
        "1. Read the golden standard answer and the chatbot answer carefully.\\\n",
        "2. Compare the chatbot answer to the golden standard answer. Check if both answers convey the same meaning.\\\n",
        "3. Assign a score for similarity on a scale of 1 to 5, where 1 is the lowest score and 5 is the highest possible score based on the Evaluation Criteria.\\\n",
        "4. Output the score.\\\n",
        "\n",
        "The chatbot answer is: {input_1}\\\n",
        "The golden standard answer is: {input_2}\\\n",
        "The score that best describes the similarity from 1 to 5 is what?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0k5Vlo2P6CaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_complete = TASK_PROMPT_1.replace(\"{input_1}\", chatbot_sentence_1).replace(\"{input_2}\", golden_answer_1)\n",
        "print(prompt_complete)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuJ9p5bl6NkJ",
        "outputId": "b20e474e-8ac5-4d8f-af86-d13cfd8837c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Given a chatbot answer and golden standard answer, your task is to determine how close they are in meaning. Please make sure to read and understand all of the instructions carefully below:\n",
            "Evaluation Criteria (Read all of the options first and then assign the score that best describes the relationship between the chatbot and golden standard answer):\n",
            "Similarity (1-5):\n",
            "- Return a 1 if the chatbot answer doesn't return anything valid.- Return a 2 if the chatbot answer gives an answer but it's not necessarily valid or close to the golden answer.- Return a 3 if the chatbot answer gives a suggestion that is far from the golden answer.- Return a 4 if the chatbot answer gives a suggestion that is pretty similar to the golden standard answer but not in the expected order.- Return a 5 if the chatbot answer means the same thing as the golden standard answer, regardless of word order.\n",
            "Evaluation Steps:\n",
            "1. Read the golden standard answer and the chatbot answer carefully.2. Compare the chatbot answer to the golden standard answer. Check if both answers convey the same meaning.3. Assign a score for similarity on a scale of 1 to 5, where 1 is the lowest score and 5 is the highest possible score based on the Evaluation Criteria.4. Output the score.\n",
            "The chatbot answer is: Professor Ming is the best professorThe golden standard answer is: The best professor is Professor MingThe score that best describes the similarity from 1 to 5 is:\n",
            "\n",
            "Given a chatbot answer and golden standard answer, your task is to determine how close they are in meaning. Please make sure to read and understand all of the instructions carefully below:\n",
            "Evaluation Criteria (Read all of the options first and then assign the score that best describes the relationship between the chatbot and golden standard answer):\n",
            "Similarity (1-5):\n",
            "- Return a 1 if the chatbot answer doesn't return anything valid.- Return a 2 if the chatbot answer gives an answer but it's not necessarily valid or close to the golden answer.- Return a 3 if the chatbot answer gives a suggestion that is far from the golden answer.- Return a 4 if the chatbot answer gives a suggestion that is pretty similar to the golden standard answer but not in the expected order.- Return a 5 if the chatbot answer and the golden standard answer convey the same meaning, even if the words or word order are different.\n",
            "Evaluation Steps:\n",
            "1. Read the golden standard answer and the chatbot answer carefully.2. Compare the chatbot answer to the golden standard answer. Check if both answers convey the same meaning.3. Assign a score for similarity on a scale of 1 to 5, where 1 is the lowest score and 5 is the highest possible score based on the Evaluation Criteria.4. Output the score.\n",
            "The chatbot answer is: Professor Ming is the best professorThe golden standard answer is: The best professor is Professor MingThe score that best describes the similarity from 1 to 5 is what?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_gpt3(prompt_complete, return_first_line = True, instruction_tuned=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "8d1ql5pZ110U",
        "outputId": "0f809073-642f-48ce-f149-6ad726d7814d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'text'\n",
            "'text'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-5b7a2f35c2fa>\u001b[0m in \u001b[0;36mrun_gpt3\u001b[0;34m(prompt, return_first_line, instruction_tuned)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-ef1a7a8677e3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_gpt3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_complete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_first_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstruction_tuned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-5b7a2f35c2fa>\u001b[0m in \u001b[0;36mrun_gpt3\u001b[0;34m(prompt, return_first_line, instruction_tuned)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Parse the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_complete = TASK_PROMPT_1.replace(\"{input_1}\", chatbot_sentence_2).replace(\"{input_2}\", golden_answer_2)\n",
        "print(prompt_complete)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feDF0yus5kVG",
        "outputId": "eba59495-5b9f-4d3a-ce51-be1240ae8169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given a chatbot answer and golden standard answer,  your task is to determine how close they are in meaning Please make sure to read and understand all of the instructions carefully below:\n",
            " \n",
            "Evaluation Criteria  (Read all of the options first and then assign the score that best describes  the relationship between the chatbot and golden standard answer):\n",
            " Similarity (1-5) :\n",
            "   - Return a 1 if the chatbot answer doesn't return anything valid\n",
            "  - Return a 2 if the chatbot answer gives an answer but it's not necessarily valid or close to the golden answer\n",
            "  - Return a 3 if the chatbot answer gives a suggestion that is far from the golden answer\n",
            "  - Return a 4 if the chatbot answer gives a pretty similar suggestion to the golden standard answer but not the expected output\n",
            "  - Return a 5 if the chatbot answer means the same thing as the golden standard answer\n",
            " \n",
            " Evaluation Steps :\n",
            " 1. Read the golden standard answer and the chatbot answer carefully\n",
            " 2. Compare the chatbot answer to the golden standard answer. Check if both answers mean the same thing,\n",
            " 3. Assign a score for similarity on a scale of 1 to 5, where 1 is the lowest score and 5 is the highest possible score based on the Evaluation Criteria.\n",
            " 4. Output the score.\n",
            " \n",
            " The chatbot answer is: A good option is Professor S\n",
            " The golden standard answer is: Professor Ming would be a good option\n",
            " The score that best describes the similarity from 1 to 5 is:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_gpt3(prompt_complete, instruction_tuned=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5wIWpfDpH_qd",
        "outputId": "329d559c-e645-4eaf-c865-de9bfa75de37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-YwVYxfjICus"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}